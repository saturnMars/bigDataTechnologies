{"cells":[{"cell_type":"code","source":["# ETL --> EXTRACT HDF5 files into Spark dataframe\nfrom pyspark.sql import SparkSession, Row\nfrom pyspark import SparkContext\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport tables\nimport h5py\n\n# Get list of all HDF5 files contained in the folder \ndef build_list_file(root_folder):\n  files = []\n  if(LIMIT_SONG != -1):\n    list_patch = dbutils.fs.ls(root_folder)[:LIMIT_SONG]\n  else:\n    list_patch = dbutils.fs.ls(root_folder)\n  \n  #Fetch file names\n  for file in list_patch:\n    files.append(\"/dbfs//FileStore/tables/songs/\" + file[1])\n  return files\n    \ndef read_hdf5(path):\n  # Fetch table objects through HDFStore\n  store = pd.HDFStore(path, mode = 'r')\n  metadata = store.get(\"metadata/songs\")\n  analysis = store.get(\"analysis/songs\")\n  year = store.get(\"musicbrainz/songs\")\n\n  # Fetch arrays through h5py metadata (+ encode strings which were encoded in an old format)\n  h5 = h5py.File(path, 'r')\n  artist_terms = pd.Series([[item.decode() for item in list(h5.get(\"metadata/artist_terms\"))]], name = \"artist_terms\")\n  similar_artists = pd.Series([[item.decode() for item in list(h5.get(\"metadata/similar_artists\"))]], name = \"similar_artists\")\n  \n  # Fetch arrays by h5py - analysis (+ turn numpy numbers into float)\n  bars_start = pd.Series([[float(item) for item in list(h5.get(\"/analysis/bars_start\"))]], name=\"bars_start\")\n  beats_start = pd.Series([[float(item) for item in list(h5.get(\"/analysis/beats_start\"))]], name=\"beats_start\")\n  sections_start =pd.Series([[float(item) for item in list(h5.get(\"/analysis/sections_start\"))]], name=\"sections_start\")\n  segments_start = pd.Series([[float(item) for item in list(h5.get(\"/analysis/segments_start\"))]], name=\"segments_start\")\n  tatums_start = pd.Series([[float(item) for item in list(h5.get(\"/analysis/tatums_start\"))]], name=\"tatums_start\")\n  \n  # THE FOLLOWING FIELDS HAVE BEEN EXCLUDED FROM THE ORIGINAL SONG FEATURES\n  # EXCLUDED (confidence arrays): bars_confidence, beats_confidence, sections_confidence, segments_confidence, tatums_confidence \n  # EXCLUDED (segments metadata): segments_pitches, segments_timbre, segments_loudness_max, segments_loudness_max_time, segments_loudness_start\n  # EXCLUDED (artist terms weights and frequency): artist_terms_freq, artist_terms_weight,\n  \n  # Merge outcomes\n  song = pd.concat([metadata, analysis,year, artist_terms, similar_artists, bars_start,beats_start, sections_start, segments_start, tatums_start], axis=1, join='outer')\n  \n  # Delete some columns \n  song = song.drop(columns = [\"idx_artist_terms\", \"idx_similar_artists\", \"audio_md5\", \"idx_bars_confidence\",\"idx_bars_start\", \"idx_beats_confidence\", \"idx_beats_start\", \"idx_sections_confidence\", \"idx_sections_start\", \"idx_segments_confidence\", \"idx_segments_loudness_max\", \"idx_segments_loudness_max_time\", \"idx_segments_loudness_start\",\t\"idx_segments_pitches\", \"idx_segments_start\", \"idx_segments_timbre\", \"idx_tatums_confidence\", \"idx_tatums_start\", \"time_signature_confidence\", \"analyzer_version\", \"key_confidence\", \"mode_confidence\", \"idx_artist_mbtags\", \"track_7digitalid\", \"release_7digitalid\", \"artist_playmeid\",\"artist_mbid\", \"artist_7digitalid\",\"analysis_sample_rate\"])\n\n  return song.values.tolist()\n\n#-----------------------------------------------------------------------------------\nROOT_FOLDER = \"/FileStore/tables/songs\"\nLIMIT_SONG = -1 #All songs\n\n# GET: Spark context\nspark = SparkSession.builder.appName(\"Milion Songs Dataset\").getOrCreate()\nsc = SparkContext.getOrCreate() \n\n# READ: HDF5 files (using RDD)\nfiles_list = build_list_file(ROOT_FOLDER)\nrdd = sc.parallelize(files_list).flatMap(lambda path: read_hdf5(path)) \n\nschema = StructType([\n  StructField(\"artist_familiarity\", DoubleType(), True),\n  StructField(\"artist_hotttnesss\", DoubleType(), True),\n  StructField(\"artist_id\", StringType(), True),\n  StructField(\"artist_latitude\", DoubleType(), True),\n  StructField(\"artist_location\", StringType(), True),\n  StructField(\"artist_longitude\", DoubleType(), True),\n  StructField(\"artist_name\", StringType(), True),\n  StructField(\"genre\", StringType(), True),\n  StructField(\"release\", StringType(), True),\n  StructField(\"song_hotttnesss\", DoubleType(), True),\n  StructField(\"song_id\", StringType(), True),\n  StructField(\"title\", StringType(), True),\n  StructField(\"danceability\", DoubleType(), True),\n  StructField(\"duration\", DoubleType(), True),\n  StructField(\"end_of_fade_in\", DoubleType(), True),\n  StructField(\"energy\", DoubleType(), True),\n  StructField(\"key\", LongType(), True),\n  StructField(\"loudness\", DoubleType(), True),\n  StructField(\"mode\", LongType(), True),\n  StructField(\"start_of_fade_out\", DoubleType(), True),\n  StructField(\"tempo\", DoubleType(), True),\n  StructField(\"time_signature\", LongType(), True),\n  StructField(\"track_id\", StringType(), True),\n  StructField(\"year\", LongType(), True),\n  StructField(\"artist_terms\", ArrayType(StringType()), True),\n  StructField(\"similar_artists\", ArrayType(StringType()), True),\n  StructField(\"bars_start\", ArrayType(DoubleType()), True),\n  StructField(\"beats_start\", ArrayType(DoubleType()), True),\n  StructField(\"sections_start\", ArrayType(DoubleType()), True),\n  StructField(\"segments_start\", ArrayType(DoubleType()), True),\n  StructField(\"tatums_start\", ArrayType(DoubleType()), True)])\n\n# Create dataframe\ndf = spark.createDataFrame(rdd,schema) \nprint(\"NUM_SONGS_LOADED: \", df.count())\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# ETL --> TRANSFORM, add and remove some columns\nfrom pyspark.sql.functions import col, size, create_map, lit, concat, element_at, when\nimport numpy as np\nfrom itertools import chain\n\n#df = spark.sql(\"SELECT * FROM SONGS\")\n\n# ADD COLUMNS: Density, Fadiness and Genre\ndf=df.withColumn('density', size(df.segments_start)/df.duration)\ndf=df.withColumn('fadiness', df.end_of_fade_in+df.duration-df.start_of_fade_out)\ndf=df.withColumn('variability', size(df.sections_start))\n\n# ADD COLUMN: Tonality (Key + Mode)\ndict_mode={1:'maj', 0:'min'}\ndict_key ={0:'DO',1:'DO#',2:'RE',3:'RE#',4:'MI',5:'FA',6:'FA#',7:'SOL',8:'SOL#',9:'LA',10:'LA#',11:'SI'}\n\nmapping_expr_key = create_map([lit(x) for x in chain(*dict_key.items())])\nmapping_expr_mode = create_map([lit(x) for x in chain(*dict_mode.items())])\n\ndf=df.withColumn(\"key_string\", (mapping_expr_key.getItem(col(\"key\"))))\ndf=df.withColumn('mode_string', (mapping_expr_mode.getItem(col(\"mode\"))))\ndf=df.withColumn('tonality', concat(df.key_string,lit(' '),df.mode_string))\n\n# Set some value to 0 when there's NaN value \ndf=df.withColumn(\"song_hotttnesss\", when(col(\"song_hotttnesss\")==\"NaN\", 0).otherwise(col(\"song_hotttnesss\")))\ndf=df.withColumn(\"artist_familiarity\", when(col(\"artist_familiarity\")==\"NaN\", 0).otherwise(col(\"artist_familiarity\")))\n\n# Final dataframe\nincluded =[\"artist_name\",\"artist_familiarity\",\"artist_hotttnesss\",\"artist_id\",\"artist_latitude\",\"artist_longitude\",\"artist_location\", \"release\",\"song_hotttnesss\",\"key_string\",\"mode_string\",\"tonality\",\"title\",\"duration\",\"loudness\",\"tempo\",\"track_id\",\"year\",\"density\",\"fadiness\",\"variability\",\"similar_artists\"]\nsongs = df.select(included)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# ETL: Transfrom (Join with another table) - Improve genre lables with \"Tagtraum genre annotation table\"\ndf = spark.sql(\"SELECT SONGS.*, GENRE.SEED_GENRE AS GENRE FROM SONGS LEFT JOIN genre ON SONGS.track_id=genre.track_id\")\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# ETL --> LOAD and save dataframe as HIVE TABLE\ndf.write.mode('overwrite').format(\"parquet\").saveAsTable(\"songs\")\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":4}],"metadata":{"name":"1. Import dataset","notebookId":994940817507328},"nbformat":4,"nbformat_minor":0}
