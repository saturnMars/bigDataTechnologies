{"cells":[{"cell_type":"code","source":["TYPE_CLUSTER = [\"popularity_type\",\"quality_of_song\",\"type_of_running\", \"quantity_of_fade\", \"song_style\", \"estimated_region\"]\nNUM_CLUSTER = [6, 4, 4, 3, 5, 10]\n\nFEATURES = [\n  [\"artist_hotttnesss\", \"artist_familiarity\",\"song_hotttnesss\"],\n  [\"variability\", \"song_hotttnesss\"],\n  [\"duration\", \"tempo\"],\n  [\"fadiness\"],\n  [\"density\", \"tempo\"],\n  [\"artist_latitude\", \"artist_longitude\"]]\n\nNAMES = [\n  [\"Average\",\"Popular\", \"Undefined\",'One-hit Wonder','Superstar','Underdog Song'],#singer_type\n  [\"Average\", \"Complex\", \"Unpopular\", \"Popular\"],\n  [\"Jogging\", \"Cardio\", \"Marathon\", \"Sprint\"], #type_of_running \n  [\"Long\", \"Short\", \"Average\"], #fading\n  [\"Rock/Hip-Hop\", \"Virtuous\", \"Dub/Instrumental\", \"Pop\", \"Ambient\"], #density \n  [\"South America\",\"Easten America \",\"Europe\",\"Middle East\",\"Oceania\",\"North, Central America\",\"North Europe\", \"Africa\",\"Western America\",\"No data\"]] \n\nGENERAL_QUERY = \"\"\" \n  SELECT track_id, artist_location, artist_latitude, artist_longitude,\n  duration, tempo, density, fadiness\n  FROM songs\n \"\"\"\n\nQUERY_HOTNESS = \"\"\" \n  SELECT track_id,\n  artist_familiarity, artist_hotttnesss, song_hotttnesss, variability\n  FROM songs WHERE artist_familiarity!=0 AND artist_hotttnesss !=0 AND song_hotttnesss!=0\n \"\"\""],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# PIPELINE --> 1. DATA INGESTION\nfrom os.path import abspath\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,when\nfrom pyspark.sql.types import IntegerType\n  \nspark = SparkSession.builder.appName(\"Milion Songs Dataset\").config(\"spark.sql.warehouse.dir\", abspath('/user/hive/warehouse/songs')).enableHiveSupport().getOrCreate()\n\ndef load_spark_dataset(QUERY_SQL):\n  spark_dataset = spark.sql(QUERY_SQL) \n  return  spark_dataset\n\n# DATA INGESTION: LOAD data and extraxt training and testing dataframe\nspark_dataset = load_spark_dataset(GENERAL_QUERY)\nspark_dataset_hotnesses = load_spark_dataset(QUERY_HOTNESS)\n\n# DATA PREPARATION: TURN \"NaN\" values into 0 values (to insert them into clustering model)\nspark_dataset = spark_dataset.withColumn(\"artist_longitude\", when(col(\"artist_longitude\")==\"NaN\", 0).otherwise(col(\"artist_longitude\")))\nspark_dataset = spark_dataset.withColumn(\"artist_latitude\", when(col(\"artist_latitude\")==\"NaN\", 0).otherwise(col(\"artist_latitude\")))\n\n# DATA PREPARATION: Add column \"features\" for the clustering algorithm (one column for the features selected)\nassembler_c1 = VectorAssembler(inputCols=FEATURES[0],outputCol='features_C1')\nassembler_c2 = VectorAssembler(inputCols=FEATURES[1],outputCol='features_C2')\nassembler_stages_hotness = [assembler_c1, assembler_c2]\n\nassembler_c3 = VectorAssembler(inputCols=FEATURES[2],outputCol='features_C3')\nassembler_c4 = VectorAssembler(inputCols=FEATURES[3],outputCol='features_C4')\nassembler_c5 = VectorAssembler(inputCols=FEATURES[4],outputCol='features_C5')\nassembler_c6 = VectorAssembler(inputCols=FEATURES[5],outputCol='features_C6')\nassembler_stages = [assembler_c3, assembler_c4,assembler_c5,assembler_c6]"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# PIPELINE --> 2. CLUSTERING (GaussianMixture)\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.clustering import GaussianMixture\n\n# DEFINE MODEL: Gaussian Mixture Model (GMM) \ngmm_c1 = GaussianMixture(k=NUM_CLUSTER[0] , featuresCol=\"features_C1\", predictionCol=TYPE_CLUSTER[0],probabilityCol='probability_' + TYPE_CLUSTER[0], seed=1)\ngmm_c2 = GaussianMixture(k=NUM_CLUSTER[1] , featuresCol=\"features_C2\", predictionCol=TYPE_CLUSTER[1],probabilityCol='probability_' + TYPE_CLUSTER[1], seed=1)\nclustering_stages_hotness = [gmm_c1,gmm_c2]\n\ngmm_c3 = GaussianMixture(k=NUM_CLUSTER[2] , featuresCol=\"features_C3\", predictionCol=TYPE_CLUSTER[2],probabilityCol='probability_' + TYPE_CLUSTER[2], seed=1)\ngmm_c4 = GaussianMixture(k=NUM_CLUSTER[3] , featuresCol=\"features_C4\", predictionCol=TYPE_CLUSTER[3],probabilityCol='probability_' + TYPE_CLUSTER[3], seed=1)\ngmm_c5 = GaussianMixture(k=NUM_CLUSTER[4] , featuresCol=\"features_C5\", predictionCol=TYPE_CLUSTER[4],probabilityCol='probability_' + TYPE_CLUSTER[4], seed=1)\ngmm_c6 = GaussianMixture(k=NUM_CLUSTER[5] , featuresCol=\"features_C6\", predictionCol=TYPE_CLUSTER[5],probabilityCol='probability_' + TYPE_CLUSTER[5], seed=1)\nclustering_stages = [gmm_c3,gmm_c4, gmm_c5, gmm_c6]\n\n# TRAINIG MODEL\npipeline_hotnesses = Pipeline(stages = assembler_stages_hotness + clustering_stages_hotness)\npipeline = Pipeline(stages = assembler_stages + clustering_stages)\n\nmodel_hotness = pipeline_hotnesses.fit(spark_dataset_hotnesses)\nmodel = pipeline.fit(spark_dataset) \nmodels = [model_hotness, model]\n\n# PREDICT CLUSTERS\nsong_labelled_hotnesses = model_hotness.transform(spark_dataset_hotnesses)\nsong_labelled_general  = model.transform(spark_dataset)\nsong_labelled = song_labelled_general.join(song_labelled_hotnesses, \"track_id\")\n\n# DISPLAY OUTCOMES (clustering models)\nindex_cluster = 0\nfor item in models:\n  for stage in item.stages:\n     if(isinstance(stage, VectorAssembler) != True):        \n        print(\"--------------------CLUSTER (type:{0})----------------------------------\".format(TYPE_CLUSTER[index_cluster]))\n        print(\"NUMER OF CLUSTER: {0} \\nCLUSTERING_FEATURES: {1}\".format(NUM_CLUSTER[index_cluster], FEATURES[index_cluster]))\n\n        # Centroid\n        centroid = stage.gaussiansDF.select(col(\"mean\").alias(\"Centroid\")).show(truncate=False)\n\n        # Count occurrances\n        print(\"Number of occurrences for each cluster\")\n        song_labelled.groupBy(TYPE_CLUSTER[index_cluster]).count().orderBy(TYPE_CLUSTER[index_cluster]).show()\n        \n        index_cluster += 1"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#PIPELINE ---> 3. CLUSTERS LABELLING \nfrom pyspark.sql.functions import lit,udf\nfrom pyspark.sql.types import FloatType\n\n# Mapper --> Go deep into the probability array\nmap_array = udf(lambda prob,i: float(prob[i]), FloatType())\n\nfor i in range(len(TYPE_CLUSTER)):\n  print(\"\\n\",TYPE_CLUSTER[i])\n  \n  # Create \"confidence column\": probability of the predicted cluster\n  song_labelled = song_labelled.withColumn(\"confidence_\"+TYPE_CLUSTER[i],map_array(col(\"probability_\"+TYPE_CLUSTER[i]), lit(col(TYPE_CLUSTER[i]))))\n\n  # Labelling \n  for cluster, name in enumerate(NAMES[i]):\n    song_labelled = song_labelled.withColumn(TYPE_CLUSTER[i], when(col(TYPE_CLUSTER[i]) == cluster, name).otherwise(col(TYPE_CLUSTER[i])))\n    print(\"LABELLED --> cluster {0} = {1}\".format(cluster, name))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# PIPELINE --> 4. Display raw outcomes of model(table way)\n\n# Build string for querying\nconfidence_labels = [\"confidence_\"+ item for item in TYPE_CLUSTER]\nfeature_labels = []\nfor type_cluster in FEATURES:\n  for feature in type_cluster:\n    if feature not in feature_labels:\n      feature_labels.append(feature)\n\n# Display\nsongs = song_labelled.select(feature_labels + TYPE_CLUSTER + confidence_labels)\ndisplay(songs)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["COLUMNS = [\"artist_location\", \"estimated_region\"] + FEATURES[5]\nTO_FILTER = \"-\"\n#display(song_labelled.select(COLUMNS).filter(song_labelled.estimated_region == TO_FILTER))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# PIPELINE --> 5. Display outcomes (graphical way)\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams[\"font.family\"] = \"DejaVu Serif\"\n\ndf = songs.toPandas()\n\nfig, axes = plt.subplots(3,2,figsize=(20, 18))\nsns.set_style(\"whitegrid\")\n\n#Plot a scatterplot for each CLUSTER\npopolarity = sns.scatterplot(x= df[FEATURES[0][0]], y=df[FEATURES[0][2]], size = df[FEATURES[0][1]], hue=df[TYPE_CLUSTER[0]], ax=axes[0][0])\npopolarity.set_title(TYPE_CLUSTER[0].upper().replace(\"_\", \" \"), fontsize=25, pad=10)\n\nquality = sns.scatterplot(x= df[FEATURES[1][0]], y=df[FEATURES[1][1]], hue=df[TYPE_CLUSTER[1]], ax=axes[0][1])\nquality.set_title(TYPE_CLUSTER[1].upper().replace(\"_\", \" \"), fontsize=25, pad=10)\n\nrunning = sns.scatterplot(x= df[FEATURES[2][0]], y=df[FEATURES[2][1]], hue=df[TYPE_CLUSTER[2]], ax=axes[1][0])\nrunning.set_title(TYPE_CLUSTER[2].upper().replace(\"_\", \" \"), fontsize=25, pad=10)\n\nfading = sns.scatterplot(x= np.linspace(0, 10, num=len(df[FEATURES[3][0]])), y=df[FEATURES[3][0]], hue=df[TYPE_CLUSTER[3]], ax=axes[1][1])\nfading.set_title(TYPE_CLUSTER[3].upper().replace(\"_\", \" \"), fontsize=25, pad=10)\n\ngenre = sns.scatterplot(x= df[FEATURES[4][1]], y=df[FEATURES[4][0]], hue=df[TYPE_CLUSTER[4]], ax=axes[2][0])\ngenre.set_title(TYPE_CLUSTER[4].upper().replace(\"_\", \" \"), fontsize=25, pad=10)\n\nregion = sns.scatterplot(x= df[FEATURES[5][1]], y=df[FEATURES[5][0]], hue=df[TYPE_CLUSTER[5]], ax=axes[2][1])\nregion.set_title(TYPE_CLUSTER[5].upper().replace(\"_\", \" \"), fontsize=25, pad=10)\n\n#fig.suptitle(\"Clusters\", fontsize=60, fontweight=\"regular\",x = 0.5)\nplt.tight_layout(pad = 3, rect=[0, 0, 1, 0.96]) #(left, bottom, right, top)\nplt.show()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#PIPELINE --> 6. Show goodness (confidence of our clusters)\n\ndf = songs.toPandas()\nplt.rcParams[\"font.family\"] = \"DejaVu Serif\"\nfig, axes = plt.subplots(nrows = 6, figsize=(20, 32))\nsns.set_style(\"whitegrid\")\nsns.set(font='DejaVu Sans')\n\ncol,row = 0,-1\n# Plot boxplots\nfor i, cluster in enumerate(TYPE_CLUSTER):\n  bx = sns.boxplot(x = df[\"confidence_\" + cluster], y = df[cluster],ax = axes[i])\n  \n  bx.set_title(cluster.upper().replace(\"_\", \" \"), fontsize=30, pad=10)\n  bx.set_xlabel(\"Confidence\",fontsize = 20)\n  bx.set_ylabel(\"CLUSTERS\",fontsize = 30, color = \"tomato\",labelpad =30)\n  bx.tick_params(axis = \"x\",labelsize = 17)\n  bx.tick_params(axis = \"y\",labelsize = 20)\n  bx.set_xlim(0.2,1)\n  \n#fig.suptitle(\"Confidence of the clusters\", fontsize=40, fontweight=\"bold\",x = 0.6)\nplt.tight_layout(pad = 3, rect=[0, 0, 1, 0.96]) #(left, bottom, right, top)\nplt.show()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# PIPELINE --> 7. Save outcomes\nclusters = song_labelled.select([\"track_id\"]+ TYPE_CLUSTER + confidence_labels)\nclusters.write.mode('overwrite').format(\"parquet\").saveAsTable(\"clusters_\")\ndisplay(clusters)"],"metadata":{},"outputs":[],"execution_count":9}],"metadata":{"name":"3. Clustering","notebookId":2924908310859602},"nbformat":4,"nbformat_minor":0}
